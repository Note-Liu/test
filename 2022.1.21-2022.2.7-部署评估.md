# Learning-MMDeploy
基础平台：Linux, RTX2080Ti, Python3.6, Torch1.8  
(为了后续使用的方便性，以下安装过程全程Docker中完成。)  
## 安装mmdetection

```shell
pip install openmim
mim install mmdet
```
安装完成后，测试:  
```python
from mmdet.apis import init_detector, inference_detector

config_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'
# download the checkpoint from model zoo and put it in `checkpoints/`
# url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
checkpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
device = 'cuda:0'
# init a detector
model = init_detector(config_file, checkpoint_file, device=device)
# inference the demo image
inference_detector(model, 'demo/demo.jpg')
```

## 安装mmdeploy

下载mmdeploy  
```bash
git clone -b master git@github.com:open-mmlab/mmdeploy.git MMDeploy
cd MMDeploy
git submodule update --init --recursive
```
Build backend support
编译onnx
```bash
pip install onnxruntime==1.8.1
```
Build custom ops
```bash
wget https://github.com/microsoft/onnxruntime/releases/download/v1.8.1/onnxruntime-linux-x64-1.8.1.tgz

tar -zxvf onnxruntime-linux-x64-1.8.1.tgz
cd onnxruntime-linux-x64-1.8.1
export ONNXRUNTIME_DIR=$(pwd)
export LD_LIBRARY_PATH=$ONNXRUNTIME_DIR/lib:$LD_LIBRARY_PATH
```
添加环境变量
```bash
echo '# set env for onnxruntime' >> ~/.bashrc
echo "export ONNXRUNTIME_DIR=${ONNXRUNTIME_DIR}" >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$ONNXRUNTIME_DIR/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```
Build on Linux

```bash
cd ${MMDEPLOY_DIR} # To MMDeploy root directory
mkdir -p build && cd build
cmake -DMMDEPLOY_TARGET_BACKENDS=ort -DONNXRUNTIME_DIR=${ONNXRUNTIME_DIR} ..
make -j$(nproc)
```
![image](./operateimg/9.png)

编译trt
官网下载TensorRT 8的tar包(本机使用的是CUDA11.2版本)
![image](./operateimg/1.png)
使用tar xzvf解压，解压后文件夹文件夹为"TensorRT-8.x.x.x"形式    
文件夹中包含bin、data、lib等文件/文件夹  
添加环境变量
```bash
version="8.x.x.x"
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:TensorRT-${version}/lib
```
安装python TensorRT wheel
```bash
cd TensorRT-${version}/python
python3 -m pip install tensorrt-*-cp3x-none-linux_x86_64.whl
```
安装Python graphsurgeon wheel  
```bash
cd TensorRT-${version}/graphsurgeon
python3 -m pip install graphsurgeon-0.4.5-py2.py3-none-any.whl
```
安装Python onnx-graphsurgeon wheel
```bash
cd TensorRT-${version}/onnx_graphsurgeon
python3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl
```
添加环境变量至bashrc
```bash
cd ${TENSORRT_DIR} # To TensorRT root directory
echo '# set env for TensorRT' >> ~/.bashrc
echo "export TENSORRT_DIR=${TENSORRT_DIR}" >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$TENSORRT_DIR/lib:$TENSORRT_DIR' >> ~/.bashrc
source ~/.bashrc
```
Build custom ops
```bash
cd ${MMDEPLOY_DIR} # To MMDeploy root directory
mkdir -p build && cd build
cmake -DMMDEPLOY_TARGET_BACKENDS=trt ..
make -j$(nproc)
```

## PyTorch模型转onnx
For example, if you want to convert the Faster-RCNN in [MMDetection](https://github.com/open-mmlab/mmdetection) to TensorRT
```bash
python ./tools/deploy.py \
    configs/mmdet/detection/detection_onnxruntime_dynamic.py \
    $PATH_TO_MMDET/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \
    $PATH_TO_MMDET/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
    $PATH_TO_MMDET/demo/demo.jpg \
    --work-dir work_dir \
    --show \
    --device cuda:0
```
执行过程中会分别show出onnx和pytorch模型的检测结果
onnx结果
![image](./operateimg/2.png)
pytorch结果
![image](./operateimg/3.png)



## PyTorch模型转tensorrt
```bash
python ./tools/deploy.py \
    configs/mmdet/detection/detection_tensorrt_dynamic-320x320-1344x1344.py \
    $PATH_TO_MMDET/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \
    $PATH_TO_MMDET/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
    $PATH_TO_MMDET/demo/demo.jpg \
    --work-dir work_dir \
    --show \
    --device cuda:0
```
执行过程中会分别show出tensorrt和pytorch模型的检测结果
tensorrt结果
![image](./operateimg/4.png) 
pytorch结果
![image](./operateimg/5.png)

在转tensorrt过程中，遇到报错
![image](./operateimg/6.png)

看起来像是没有Build custom ops，但我重复编译了几次还是这个bug，后来意识到build文件没有clear，clear后重新编译就ok了，记录一下。o(╯□╰)o



## TensorRT模型评估
数据集：COCO2017
```shell
python tools/test.py \
    configs/mmdet/detection/detection_tensorrt_dynamic-320x320-1344x1344.py \
    $PATH_TO_MMDET/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \
    --model ${BACKEND_MODEL_FILES} \
    --speed-test \
    --device cuda:0
```
使用detection_tensorrt_dynamic-320x320-1344x1344.py进行评估
![image](./operateimg/10.png)

输出15.08FPS
官网使用NVIDIA tesla T4，800*1344的input，FPS为11.35
![image](./operateimg/11.png)

